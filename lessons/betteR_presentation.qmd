---
title: "betteR!"
subtitle: "What tools can we, at EPA, use to make better use of R"
format: revealjs
execute: 
  echo: true
  cache: true
---

## Overview

- What can we do on our current PCs?
- What can we do using other computational infrastructure?
- What additional tools, that compliment R, do we have available?

# Better R on your Current PC

## Speed up iteration

- Doing things over and over again
- Example

```{r eval=FALSE}
my_vec <- NULL
set.seed(42)
for(i in 1:5000){
  my_vec_slow <- c(my_vec_slow, mean(rnorm(100)) 
}
```

## Speed up iteration

- Speed it up by pre-allocating memory
- Example

```{r eval=FALSE}
my_vec <- numeric(5000)
set.seed(42) 
for(i in 1:5000){
 my_vec[i] <- mean(rnorm(100)) 
}
```

## Speed up iteration

```{r}
library(microbenchmark)
microbenchmark(
  not_allocated = {
    my_vec_slow <- NULL
    set.seed(42)
    for(i in 1:5000){
      my_vec_slow <- c(my_vec_slow, mean(rnorm(100))) 
    }},
  allocated = {
    my_vec <- numeric(5000)
    set.seed(42)
    for(i in 1:5000){
      my_vec[i] <- mean(rnorm(100)) 
    }}, 
  times = 5, unit = "seconds") |>
  summary() |>
  select(expr, min, mean, median, max)
```

## Parallel processing

```{r}
library(future)
library(future.apply)
plan(multisession, workers = availableCores() - 1)
microbenchmark(
  serial = {
    my_vec <- numeric(5000)
    set.seed(42)
    for(i in 1:5000){
      my_vec[i] <- mean(rnorm(100)) 
    }}, 
  parallel = {
    my_vec_p <- numeric(5000)
    my_vec_parallel <- unlist(future_lapply(my_vec_p, 
                                            function(x) mean(rnorm(100)), 
                            future.seed = 42))
    },
  times = 5, unit = "seconds") |>
  summary() |>
  select(expr, min, mean, median, max)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
plan(sequential)
```
## Parallel processing

```{r}
plan(multisession, workers = availableCores() - 1)
microbenchmark(
  serial = {
    my_vec <- numeric(100000)
    set.seed(42)
    for(i in 1:100000){
      my_vec[i] <- mean(rnorm(100)) 
    }}, 
  parallel = {
    my_vec_p <- numeric(100000)
    my_vec_parallel <- unlist(future_lapply(my_vec_p, 
                                            function(x) mean(rnorm(100)), 
                            future.seed = 42))
    },
  times = 5, unit = "seconds") |>
  summary() |>
  select(expr, min, mean, median, max)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
plan(sequential)
```
## Data Input/Output (I/O)

- Reading and writing data can take time
- Data size can matter
  - local or other services
- the `.csv` is great, but ...
  - not efficient storage
  - can be slow to read and write.
- Solutions: 
  - Different packages
  - Different formats
  
## Data Input/Output (I/O)

- Start with biggish `.csv` file ([download])
- `big_nla.csv` = 33.2 MB

```{r}
library(dplyr)
microbenchmark(
  read.csv = {big_nla <- read.csv("big_nla.csv")},
  times = 5, unit = "seconds"
) |>
  summary() |>
  select(expr, min, mean, median, max)
```

## Data Input/Output (I/O): Read csv

```{r messages = FALSE, warnings = FALSE}
library(readr)
library(data.table)
library(arrow)
microbenchmark(
  base = {big_nla <- read.csv("big_nla.csv")},
  readr = {big_nla <- read_csv("big_nla.csv")},
  data.table = {big_nla <- fread("big_nla.csv")},
  arrow = {big_nla <- read_csv_arrow("big_nla.csv")},
  times = 5, unit = "seconds"
) |>
  summary() |>
  select(expr, min, mean, median, max)
```

## Data Input/Output (I/O): Write

```{r messages = FALSE, warnings = FALSE}
microbenchmark(
  base = {write.csv(big_nla, "base_big_nla.csv")},
  readr = {write_csv(big_nla, "readr_big_nla.csv")},
  data.table = {fwrite(big_nla, "datatable_big_nla.csv")},
  arrow = {write_csv_arrow(big_nla, "arrow_big_nla.csv")},
  times = 2, unit = "seconds"
) |>
  summary() |>
  select(expr, min, mean, median, max)
```

## Data Input/Output (I/O): Write other formats

```{r messages = FALSE, warnings = FALSE}
microbenchmark(
  base = {save(big_nla, file = "big_nla.rda")},
  arrow_parquet = {write_parquet(big_nla, "big_nla.parquet")},
  arrow_feather = {write_feather(big_nla, "big_nla.feather")},
  times = 5, unit = "seconds"
) |>
  summary() |>
  select(expr, min, mean, median, max)
```

## Data Input/Output (I/O): File size

```{r}
library(tibble)
file_sizes <- tibble(files = list.files(pattern = "big_nla"),
                     size_mb = file.size(list.files(pattern = "big_nla"))/1000000)
file_sizes |>
  arrange(size_mb)
```

## Data Input/Output (I/O): Read all

```{r}
microbenchmark(
  base_csv = {big_nla <- read.csv("big_nla.csv")},
  readr_csv = {big_nla <- read_csv("big_nla.csv")},
  data.table_csv = {big_nla <- fread("big_nla.csv")},
  arrow_csv = {big_nla <- read_csv_arrow("big_nla.csv")},
  base_rda = {load("big_nla.rda")},
  arrow_parquet = {read_parquet("big_nla.parquet")},
  arrow_feather = {read_feather("big_nla.feather")},
  times = 5, unit = "seconds" 
) |>
  summary() |>
  select(expr, min, mean, median, max) |>
  arrange(median)
```


## Current PC: When and and how much to worry about optimisation

- Your old code took 1 minute, new code takes 10 seconds
  - Yeah!  Our code is 6 times faster
- You spent one hour to get this improvement
  - That is 59 minutes you could have used for something else

For a good read on the subject of improving performance of R code: <https://adv-r.hadley.nz/perf-improve.html>

# Better R using other infrastructure

## Get a new PC

- EISD has limits
- ORD can purchase new PCs
- You can use VMs
- Issues
  - Maintenance and Updates
  - What happens in 5-10 years?
  - Still has EPA Image

## High End Scientific Computing

- Atmos, "on premise" HPC in RTP
- Yearly proposals
- Paid by WCF
- HPC less competitive recently
- Linux only
- Lots of overhead to learn
- Does have RStudio available

## EPA DMAP Analytics Platform

- [DMAP Sharepoint](https://usepa.sharepoint.com/sites/oei_Work/edapservicecenter/default.aspx)
- [DMAP Scoping Form](https://forms.office.com/Pages/ResponsePage.aspx?id=s3iziEhnZ0is-Xaqy-ymp1GzdIs8T4FKixGaIGTfTcVUNkRVUlFMR0NZTkhFTkExNUM0Vzk2OFdaRyQlQCN0PWcu)
- There can be a cost, but if total runs are < $500.00 seems to fly under the radar.
- [Demo](https://anote.epa.gov/#/userhome)

# Better R using additional tools

## Git and GitHub Enterprise

- Git for version control
  - [Link to request install]()
- GitHub for collaboration (and more)
  - No install, it isn't software, its a website/service!
  - [USEPA on GitHub](https://github.com/USEPA)
  - [USEPA GitHub Guidance](https://work.epa.gov/it-architecture/github-guidance)

## Git and GitHub Enterprise: Demo's
  - Use git and GitHub with RStudio
  - Provisional Data Example
    - <https://github.com/usepa/fedsdata>
    - <https://github.com/usepa/high_res_cyano>
    - <https://github.com/usepa/provisional_habs>

## Other DMAP Tools

- Large data storage on AWS
- Data sharing via the Data Commons
- Posit Connect
  - Enterprise tool for sharing R content
    - Quarto/Markdown documents
    - General R output
    - Shiny app hosting
    - Plumber APIs
    
## Other DMAP Tools: Demo

- Make a Shiny app and publish to Posit Connect

## Parting Shot

- Sign up for regular and automatic R/RStudio updates
  - Sign Up Form: <https:/forms.office.com/g/seT512wZi8> 

